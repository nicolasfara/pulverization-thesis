\chapter{Background}
\label{chap:background}

This chapter is organized into three sections. The first section provides an overview of the current layered and heterogeneous infrastructure
defined by the could-edge interplay. The second section describes the problem of the deployment independence of the applications by giving an
overview of the actual frameworks and methodologies. Finally, the third section describes the pulverization methodology in aggregate computing and
cyber-physical systems.

\section{Layered and heterogeneous infrastructure}
\label{sec:layered-heterogeneous-infrastructure}

Nowadays, electronic devices are capable of generating vast amounts of data, from measuring natural phenomena to human behavior.
The growth of the Internet of Things (IoT) is expected to connect virtually all objects, leading to a need to transfer, store, and process
unprecedented amounts of data.

Cloud computing has become an accessible platform for storing and processing data for a variety of applications, including IoT devices. It offers
flexibility and low initial costs, but its adoption has exposed limitations in fulfilling requirements for real-time, low-latency, and mobile
applications. Centralized cloud data centers are often physically and logically distant from the client, requiring multiple hops and causing delays
and consuming network bandwidth.

The adoption of cloud computing and the increasing ability of edge devices to generate and consume heterogeneous data requires new distributed
computing infrastructures that can handle diverse application requirements. Recent computing infrastructures that enact applications at edge devices
have improved response time and reduced bandwidth use. Fog computing has emerged as a paradigm that combines the ability to run localized
applications at the edge with the high capacity of the cloud, supporting the heterogeneous requirements of both small and large applications through
multiple layers of the computational infrastructure.

Taking advantage of the characteristics of the edge-cloud continuum enables many opportunities in the Internet of Things field, like
having systems that comply with requirements such as security, data locality, real-time computation, etc.
Nevertheless, the deployment of applications in this infrastructure is still a challenge and several approaches have been
proposed to address this issue~\cite{BITTENCOURT2018134}.

In the following section will be provided an overview of the main aspects and challenges that demonstrate the suitability of combining edge, fog,
and cloud computing for various applications used by the Internet of Things.

\subsection{Cloud Fog and Edge interplay}
\label{sec:cloud-fog-edge-interplay}

This section introduces the concepts and terminology of cloud, fog, and edge computing, discussing their main characteristics and finally, how
those paradigms can be combined to provide a more flexible and efficient solution for a wide range of applications.

\subsubsection{Cloud computing}

Over the last decade, cloud computing has become a widely adopted computing paradigm for many applications due to its dynamic characteristics such as elasticity and pay-per-use (achievable via virtualization and containerization), reaching a mature state.

Cloud providers offer on-demand computing through three main models, which are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and
Software as a Service (SaaS)~\cite{armbrust2010view}. IaaS provides users with remote access to computing power as a service, while PaaS offers a
platform for software development with necessary libraries and databases to deploy and run applications, and SaaS provides software that relies on
cloud providers' infrastructure to offload computing and/or data. The concept of Everything as a Service (XaaS) has emerged, which
includes a wide variety of cloud service levels.

Cloud services operate under a Service Level Agreement (SLA) that determines the services offered and the costs for using them. Common pricing models
include charging by time unit, amount of data transfer, and the number of requests. Cloud computing's features of elasticity, ubiquitous access, and
on-demand provisioning make it an attractive option, allowing for lower upfront investments and faster time to market, with reduced
operational costs.

\subsubsection{Fog computing}

The evolution of hardware in personal devices has increased computing capacity at the edge and the size of mobile devices has shrunk, allowing them
to run applications with reasonable complexity and quality of service (QoS). As a result, distributed computing paradigms are being utilized, where
edge devices are used to run applications and store data.

Fog computing creates a bridge between edge devices and the cloud, and introduces a hierarchy of computing capacity, with fog nodes, cloudlets, or
micro data centers located between the edge and the cloud. This hierarchy can be spread throughout the network, with nodes higher in the hierarchy
having larger computing capacity and serving more users, while nodes lower in the hierarchy are closer to the edge and have lower communication
delays.

The computing hierarchy in the fog infrastructure can offer a wider range of service levels, supporting applications that cannot be supported by
cloud computing alone. A fog infrastructure can handle applications with a variety of QoS requirements, as applications can run at a hierarchy
level that provides adequate processing capacity and meets latency requirements. Another consequence of the use of processing closer to the edge is
to reduce (aggregate) bandwidth use in the network along the path between the edge and the cloud.

\subsubsection{Edge computing}

Edge computing is a distributed computing paradigm that brings computation and data storage closer to where it is needed, reducing the distance that
data must travel and minimizing the latency for applications that require quick responses. Edge computing evolved from the growth of mobile devices
and the hardware evolution of personal devices.

The combination of higher computing capacity and edge networks enabled distributed computing paradigms that propose the utilization of edge devices
to run applications and store data. Edge computing is characterized by the use of devices such as smartphones, tablets, and IoT sensors and actuators
as sources of computational and storage resources. These devices have limited computational capacity and battery life but can support application
execution and storage capabilities.

Edge computing is expected to enable new applications in areas such as augmented reality, autonomous vehicles, and smart cities, among others, by
allowing data processing and analysis to occur closer to the data source, reducing response time and network congestion. Edge computing also enables
the collection of data from a variety of sources and the aggregation of data at the edge for further processing, analysis, and decision-making.

\paragraph*{}

Fog computing and edge computing are two related but distinct concepts in the field of distributed computing. However, they are often used
interchangeably or confused with one another, which can lead to misunderstandings and miscommunication.

One reason for the confusion is that both fog and edge computing refer to distributed computing infrastructures that process data closer to where it
is generated, such as on the edge of the network. They both aim to reduce network latency and bandwidth consumption by processing data locally rather
than sending it to a centralized cloud server.

However, the main difference between fog and edge computing lies in the level of hierarchy at which they operate. Edge computing typically refers to
processing that occurs at the outermost layer of the network, closer to end-user devices and sensors. It involves lightweight computing devices and
microservices that are often embedded in sensors, smartphones, or IoT devices.

Fog computing, on the other hand, involves a hierarchy of computing nodes that are distributed between the edge and the cloud. These nodes, also
known as fog nodes, cloudlets, or micro data centers, can be located at access points, routing devices in the network, or even at the core of the
network. The idea is to provide a distributed infrastructure that can handle data processing and analysis at different levels of the network
hierarchy while minimizing latency and bandwidth usage.

In conclusion, while fog computing and edge computing share similar goals and concepts, they are distinct in their approach and level of hierarchy.

\subsubsection{Edge-cloud continuum problems}

The edge-cloud continuum presents many challenges that must be addressed to optimize its performance. One of the primary issues is
managing the resources distributed across the continuum in a way that ensures efficient resource utilization and maintains QoS levels. This is
particularly challenging due to the heterogeneity of devices and applications that comprise the continuum, as well as the dynamic nature of the
network topology caused by device mobility and varying application requirements.

The movement of services in the Edge-Cloud infrastructure is an important consideration due to the inherent heterogeneity of the devices and
applications in the system. As edge and fog computing become more prevalent, there is a greater need for services to move between devices in the
hierarchy to optimize the use of resources and provide the required Quality of Service (QoS). However, managing the automatic adaptation of
services to different deployment locations while considering resource constraints at each level of the infrastructure is a major
challenge. Additionally, the heterogeneous network topology and frequent changes in device mobility and application requirements make it even more
complex.

% - New section ---------------------------------------------------------------

\section{Deployment independence}
\label{sec:deployment-independence}

The advantages of integrating different computing paradigms, such as cloud and edge computing, have already been acknowledged by numerous
industry and academia-based initiatives, one example is the OpenFog Consortium~\footnote{\url{https://opcfoundation.org/markets-collaboration/openfog/}}.

The cloud-edge computing integration is an open research topic since each of the two paradigms has its use cases and advantages. The cloud
computing paradigm is well suited for large-scale applications that require high computational power and storage capacity. On the other hand, edge
computing is well suited for applications that require low latency and high reliability, such as autonomous vehicles, smart cities, and industrial
automation. The integration of the two paradigms can provide a more flexible and efficient solution for a wide range of applications.
Nevertheless, the integration of the two paradigms is not trivial, and different approaches can be used to tackle this problem.

In this context, we refer to ``deployment independence'' as the ability of an application to be deployed on any computing infrastructure by
separating the business logic from deployment and infrastructure aspects. In this way, the system logic can be developed without considering the
underlying infrastructure, since they are orthogonal aspects. This approach, on the one hand, allows for better-engineered systems where aspects of
development and deployment are separated; while on the other hand, one can make the best use of the available infrastructure according to the
dynamics of the system.

The following section will review the main methodologies that are in the literature and aspire to develop systems that integrate cloud-edge
infrastructure.

\subsection{Actual frameworks and methodologies}
\label{sec:actual-frameworks-methodologies}

Many frameworks and methodologies have been proposed in the literature to handle the edge-cloud continuum problem. The different approaches
proposed vary in complexity and use cases, each trying to solve a specific problem.

Among the methodologies and frameworks worth mentioning is osmotic computing which operates in the IoT environment focusing on a three-tier
architecture by leveraging microservices that can be moved around the infrastructure and frameworks such as DR-BIP and DReAM that are based on the
concept of "motif" and interaction rules and reconfiguration rules to manage system deployment.

The following is an overview of how these two methodologies work, highlighting their main features and how they try to solve the integration problem.

\subsubsection{Osmotic computing}

Osmotic computing~\cite{8781958} utilizes a concept known as a MEL (microelement) to encompass resources, services, and data. In the realm of IoT,
MELs can be structured as a graph and relocated across various infrastructures based on factors such as cost, security, privacy, and performance.
MELs encapsulate four distinct elements: microservices that provide specific functionality, microdata representing the flow of information to and from
sensors or actuators, microcomputing that performs various computational tasks using real-time and historic data, and microactuators that control the
state of physical resources using actuators at the network edge.

Each application can be decomposed into (cooperative) subprograms to improve deployability and scalability. This decomposition, in osmotic computing,
defines several interacting MELs, which are atomic entities providing simple functionalities. A graph of MELs can include several microservices
(MS) and microdata (MD) combined to provide a specific behavior. In osmosis, containers (or virtual components) are used to deploy dynamically and
support the migration of MELs across heterogeneous systems.

In osmotic computing, the computing environment is divided into three layers: cloud data centers (L1), edge systems and micro data centers (L2), and
IoT devices (L3).
At L3, the IoT devices capture raw data from the environment at a fixed frequency or by events. The L2 layer is composed of network devices such as
routers, switches and gateways, supported by protocols like \emph{OpenFlow} or hardware that enables network components to be accessed remotely.
Finally, L1 is composed of data centers, which are large-scale computing facilities that provide a large number of computing resources and storage
capacity.
The L2 layer can collect the data coming from devices at L3 enabling the collection of raw data and performing some computations before transferring
these data to L1.

Osmotic computing is an extension of elastic resource management, in which the deployment and migration strategies of microelements (MELs) can change
over time based on changing infrastructure and application requirements. Osmotic computing automates the configuration and reconfiguration of MELs
based on factors such as quality of service, security, and runtime perturbations.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/osmotic-architecture.png}
	\caption[Osmotic computing architecture reference]{Osmotic computing architecture reference. Picture taken from~\Cite{8781958}.}
	\label{fig:osmotic-computing}
\end{figure}

The purpose of an osmotic platform is to balance the needs of both the infrastructure and the applications by automatically relocating microservices
to appropriate deployment locations. This approach focuses mainly on systems that are centrally managed and coordinated.

\subsubsection{DR-BIP and DReAM framework}

The \emph{Dynamic Reconfigurable BIP} framework (DR-BIP)~\cite{10.1007/978-3-030-03424-5_20} includes three main aspects of dynamism: (I) the ability
to describe parametric system coordination for an arbitrary number of instances of component types, (II) the ability to add/delete components and
manage their interaction rules depending on dynamically changing conditions, and (III) allow services to seamlessly continue their activity on any
available device or computer (fluid architectures~\cite{taivalsaari2014liquid}).
The \emph{DR-BIP} framework is an extension of the \emph{Behavioral Interconnection Protocol} (BIP)~\cite{5719588} and
\emph{Dy-BIP} (a former extension that support dynamic interactions)~\cite{10.1007/978-3-642-30564-1_1}.

The DR-DIP framework provides support for runtime changes in the system, including component creation and removal, migration between motifs, and both
programmed and triggered reconfiguration.
The use of motifs allows components to interact with others based on their behavior and interaction rules within their new
motif, providing a flexible framework for coordination. The platform shares similarities with DReAM~\cite{de2020dream}, but the use of constraints
allows for more expressive coordination.

DR-BIP uses motifs as the basic unit for describing dynamic architectures (see~\Cref{fig:motif-concept}). Each motif includes the behavior of
components, the rules for interaction between components, and the rules for reconfiguring the motif, including adding, removing, or moving components.
Motifs are structurally organized as the deployment of component instances on a logical map.
Maps are arbitrary graph-like structures consisting of interconnected positions. Deployments relate components to positions on the map.
The definition of the motif is completed by two sets of rules: (I) the interaction rules, which define the behavior of the components and the
interaction between them, and (II) the reconfiguration rules, which define the conditions under which the motif can be reconfigured.

Systems are defined as collections of motif instances, each of which can evolve independently or in coordination with other motifs through shared
components or inter-motif reconfiguration rules. Inter-motif reconfiguration rules also allow for the creation and deletion of motif instances and
the exchange of components between motifs.

DR-BIP's behavior in a motif-based system is defined in a compositional way, where every motif has its own set of interactions determined by its
local structure. These interactions remain constant until the motif executes a reconfiguration action. In the absence of reconfigurations, the system
maintains a fixed architecture and operates like a normal BIP system. Interactions do not affect the architecture, while system and/or motif
reconfigurations change the architecture, but do not impact components, meaning running components retain their state, even though new components may
be added or removed.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/motif-concept.png}
	\caption[Motif-based systems]{Motif-based System Concept. Picture taken from~\cite{10.1007/978-3-030-03424-5_20}.}
	\label{fig:motif-concept}
\end{figure}

% - New section ---------------------------------------------------------------

\section{Pulverization in Aggregate Computing and CPS}
\label{sec:pulverization-aggregate-computing-cps}

This section presents a brief overview of the state of the art in the field of aggregate computing and cyber-physical with a main focus on the
pulverization and which problems try to solve.

Self-organizing systems are a way of engineering distributed intelligence in which the system's global behavior and structure are achieved through
the continuous interaction of simple individual components. This approach allows for inherent adaptation to unexpected or unforeseeable situations
and has been applied in various contexts, such as human social behavior, and swarm robotics.

Artificial self-organizing systems are software-based systems that regulate their internal structures and behavior without external control, often by
mimicking the self-organization mechanisms observed in nature. Self-organizing approaches are applied to distributed cyber-physical systems (CPS),
where individual system components interact with each other based on physical proximity to collect and process information generated by distributed
sensors and use it to control the system behavior.

However, recent advances in technology have made modern CPS increasingly large-scale, heterogeneous, and dynamic, which makes it challenging to
engineer distributed intelligent systems that can be deployed in different contexts and exploit available resources opportunistically. To address
this problem, a framework based on the pulverization approach is proposed~\cite{fi12110203}, which breaks the overall system behavior into tiny
pieces of computation linked to sensors, actuators, and neighboring components. These sub-components can be deployed and wired separately, allowing
for a separation of concerns between the self-organization logic and the deployment context.

The pulverization approach can be implemented in the framework of Aggregate Computing~\cite{beal2015aggregate}, where global self-organizing behavior
can be specified declaratively by composing pure functions expressing increasingly complex distributed algorithms.
This approach allows for the design of distributed adaptive behavior for large-scale CPS that can be deployed in a deployment-independent way,
meaning that the behavioral description of the self-organization logic remains unchanged regardless of the specifics of the deployment context.

The pulverization approach was exercised simulating a CPS whose aim is to reduce the contribution of household winter heating to air pollution by
imposing a custom maximum temperature relative to the level of particulate matter (PM) in the area surrounding the household~\cite{fi12110203}.
The system implements the functionality in a self-organization fashion, where there isn't a central coordinator and the system autonomously organizes
its behaviour even in the face of disturbance.
The goal of the experiment is to show that via the pulverization approach, the system's business logic, defined once, can be reused in different
deployment schemes, preserving its functional behavior.

Another initial research effort~\cite{9599177} was made by combining the pulverization approach with the multi-tier programming
paradigm~\cite{weisenburger2020survey}.
The multi-tier programming paradigm defines a distributed architecture in a single compilation unit with a single language. Once the program is
specified, the compiler (or the runtime) is responsible for splitting the computation among different peers.
A language that supports multi-tier programming is \emph{ScalaLoci}~\cite{weisenburger2018distributed, weisenburger2020implementing}, a type-safe
multi-tier language hosted in Scala.
A ScalaLoci application is structured through \emph{peers} and \emph{ties,}where peers abstract over the locations representing the components of an
application, while the ties define the connection between peers. Only tied peers can communicate with each other.
The example provided in~\cite{9599177} shows how the pulverization approach can be fitted into the multi-tier programming paradigm, by defining
a logical node as a peer which in turn is composed of a set of peers representing the pulverized device.
Moreover, the example shows the conjunction of ScaFi, a Scala internal DSL that can run on the JVM or in the browser and ScalaLoci showing
how this could be the foundation stone of a unified framework living in the Scala ecosystem.
Finally, the example shows how different deployment schemes can be defined by changing the ties between peers by preserving the functional behaviour
of the system.
